"""
Base Pipeline Interface

All pipelines inherit from this. Pure data in, results out.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Optional
from datetime import datetime
from enum import Enum


class Severity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class PipelineResult:
    """Result from a pipeline execution."""
    pipeline: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    data: dict = field(default_factory=dict)
    alerts: list = field(default_factory=list)
    
    def has_alerts(self) -> bool:
        return len(self.alerts) > 0


@dataclass 
class Alert:
    """An alert generated by a pipeline."""
    name: str
    message: str
    severity: Severity
    source: str
    value: Any
    threshold: Optional[Any] = None
    timestamp: datetime = field(default_factory=datetime.utcnow)


class Pipeline(ABC):
    """
    Base class for all analysis pipelines.
    
    Pipelines are pure - they:
    - Take data as input
    - Return results as output  
    - Have no side effects
    - Know nothing about Kafka, databases, or metrics
    """
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Unique name for this pipeline."""
        pass
    
    @abstractmethod
    def process(self, data: dict) -> PipelineResult:
        """
        Process a single data point.
        
        Args:
            data: Telemetry data dict with keys like:
                  - device_id: str
                  - metric_type: str  
                  - value: float
                  - timestamp: str
                  
        Returns:
            PipelineResult with any findings/alerts
        """
        pass
    
    def process_batch(self, batch: list[dict]) -> list[PipelineResult]:
        """Process multiple data points."""
        return [self.process(data) for data in batch]
